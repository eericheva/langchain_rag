{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMpmZIFXHJDJ82HWs5DeOqj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d72d8390ca3b4f7f9a73df06dc2879eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f8c0f672bbe4572b61ca7249a4d760a",
              "IPY_MODEL_8803b4542dfb4a49afc9b8439477202d",
              "IPY_MODEL_865ab1042a7f4e36aa325979e44b6a5f"
            ],
            "layout": "IPY_MODEL_3cde8d76833a4f2c982c192061ff87aa"
          }
        },
        "6f8c0f672bbe4572b61ca7249a4d760a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0f40e4e00834e7dbfad866ac3b2d230",
            "placeholder": "​",
            "style": "IPY_MODEL_1e32f0f3f6664950905359a6af3c3a28",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8803b4542dfb4a49afc9b8439477202d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6ed4243fe854204a1dd0d41c5d34085",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec02f4953837494da0fef045252b0bd8",
            "value": 2
          }
        },
        "865ab1042a7f4e36aa325979e44b6a5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59e609526ad6478fb20ffe0ca45b78de",
            "placeholder": "​",
            "style": "IPY_MODEL_37873e8c9c4e4d9a833824e659abe790",
            "value": " 2/2 [00:25&lt;00:00, 10.56s/it]"
          }
        },
        "3cde8d76833a4f2c982c192061ff87aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0f40e4e00834e7dbfad866ac3b2d230": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e32f0f3f6664950905359a6af3c3a28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6ed4243fe854204a1dd0d41c5d34085": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec02f4953837494da0fef045252b0bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "59e609526ad6478fb20ffe0ca45b78de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37873e8c9c4e4d9a833824e659abe790": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d3dbbcda80640a5ba007addc0ad48de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_45f5f8e7eaa642c5a10f1cbaa81e61c8",
              "IPY_MODEL_165e769fba674ec6aaf816fe2bd52865",
              "IPY_MODEL_9dff6fcfd534420cb709a104f460db79"
            ],
            "layout": "IPY_MODEL_901a2c1fcfc54e2d888a95e15e21ec0e"
          }
        },
        "45f5f8e7eaa642c5a10f1cbaa81e61c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_571bf27c1cb74bcd89801ce0ef6253b1",
            "placeholder": "​",
            "style": "IPY_MODEL_cab8458a5c0543969870cf3ee402b449",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "165e769fba674ec6aaf816fe2bd52865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_226748a21c914a5dab07b781d333284a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_67e18d2e713c4bf785dedff12eb3a2e1",
            "value": 2
          }
        },
        "9dff6fcfd534420cb709a104f460db79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7f9e8e61a254d5c8b3e856ef07c52a7",
            "placeholder": "​",
            "style": "IPY_MODEL_855dc8e6038146708771a7d492878dce",
            "value": " 2/2 [00:25&lt;00:00, 10.54s/it]"
          }
        },
        "901a2c1fcfc54e2d888a95e15e21ec0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "571bf27c1cb74bcd89801ce0ef6253b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cab8458a5c0543969870cf3ee402b449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "226748a21c914a5dab07b781d333284a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67e18d2e713c4bf785dedff12eb3a2e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7f9e8e61a254d5c8b3e856ef07c52a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "855dc8e6038146708771a7d492878dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eericheva/langchain_rag/blob/dev/tutorials/start_here.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic and simple first RAG tutorial\n",
        "- *the same but in as python script format: [`start_here.py`](https://github.com/eericheva/langchain_rag/blob/main/tutorials/start_here.py)*\n",
        "- V1: FULL RAG = RETRIEVER + GENERATOR with **`create_stuff_documents_chain`** from `langchain.chains.combine_documents` and **`create_retrieval_chain`** from `langchain.chains.retrieval`\n",
        "- V2: FULL RAG = RETRIEVER + GENERATOR with **`RetrievalQA.from_chain_type`** from `langchain.chains.retrieval_qa.base`\n",
        "- V3: FULL RAG = RETRIEVER + GENERATOR with **Runnable Sequences**\n",
        "\n",
        "*Full repo with RAG hints and scripts [eericheva/langchain_rag](https://github.com/eericheva/langchain_rag/tree/main)*"
      ],
      "metadata": {
        "id": "-pA8RG9pfLfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community langchain_core\n",
        "!pip install huggingface_hub\n",
        "!pip install sentence-transformers\n",
        "!pip install pypdf\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2bwwDf0Qps8",
        "outputId": "eaf56fc3-9c5f-47f8-c7ab-23f42f705990"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.10/dist-packages (0.2.17)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.85)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (24.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.5.82)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n",
        "# shutil.rmtree('/HDD/models/HuggingFaceH4')\n",
        "# shutil.rmtree('/HDD/models/intfloat')\n",
        "# os.makedirs(\"/langchain_rag_data/raw_docs\")\n",
        "\n",
        "# import os\n",
        "# from tqdm import tqdm\n",
        "# from operator import itemgetter\n",
        "# import logging\n",
        "# from google.colab import userdata\n",
        "# import inspect"
      ],
      "metadata": {
        "id": "iLsmBKT6YSPh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f6w69Cx3QbZV"
      },
      "outputs": [],
      "source": [
        "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "# from langchain.chains.retrieval import create_retrieval_chain\n",
        "# from langchain.chains.retrieval_qa.base import RetrievalQA\n",
        "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "# from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "# from langchain_community.vectorstores import FAISS\n",
        "# from langchain_community.document_loaders import pdf\n",
        "# from langchain_core.output_parsers import StrOutputParser\n",
        "# from langchain_core.prompts import PromptTemplate\n",
        "# from langchain_core.runnables import RunnableLambda\n",
        "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# from huggingface_hub import hf_hub_download, snapshot_download"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "########### LOGER ###########\n",
        "logger = logging.getLogger(\"langchain_rag\")\n",
        "logger.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter(\n",
        "    fmt=\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        ")\n",
        "handler = logging.StreamHandler()\n",
        "handler.setFormatter(formatter)\n",
        "logger.handlers.clear()  # to avoid doubling in logger output\n",
        "logger.addHandler(handler)\n",
        "logger.propagate = False  # to avoid doubling in logger output"
      ],
      "metadata": {
        "id": "2bsIJ2fRUNr1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INITIAL SETUP\n",
        "\n",
        "Setup `Config` with your tokens, key and setup params: vectorstore type models, local paths, models etc"
      ],
      "metadata": {
        "id": "fBPial8cRO1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "########### KEYS AND TOKENS ###########\n",
        "# (Optional) LangSmith for closely monitor and evaluate your application. https://docs.smith.langchain.com/\n",
        "# go to the https://smith.langchain.com/settings, and create your oun LANGCHAIN_API_KEY\n",
        "LANGCHAIN_API_KEY = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "# (Optional) If you want to use OpenAI models,\n",
        "# go to the https://platform.openai.com/api-keys, and create your oun OPENAI_API_KEY\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "# (Optional) If you want to use HuggingFaceHub:\n",
        "# go to the https://huggingface.co/settings/tokens, and create your oun HUGGINGFACEHUB_API_TOKEN\n",
        "HUGGINGFACEHUB_API_TOKEN = userdata.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
      ],
      "metadata": {
        "id": "2jnlK-7WRIY-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "import os\n",
        "\n",
        "from huggingface_hub import hf_hub_download, snapshot_download\n",
        "\n",
        "class Config:\n",
        "    ########### SETUP ###########\n",
        "    source_path = \"/langchain_rag_data\"\n",
        "    RELOAD_VECTORSTORE = False  # True if you want to recreate new vector store with new embedding or new documents\n",
        "    # False, if you want to restore vectorstore from dump\n",
        "    DEVICE_EMB = \"cpu\"  # \"cpu\" stands for cpu, \"cuda:1\"\n",
        "    DEVICE_GEN = 1  # -1 stands for cpu\n",
        "\n",
        "    VECTORSTORE2USE = \"FAISS\"  # \"FAISS\", \"CHROMA\"\n",
        "    # models = repo names from hugginface_hub\n",
        "    HF_EMB_MODEL = \"google/gemma-2b-it\"  # model for embedding documents\n",
        "    HF_LLM_NAME = \"google/gemma-2b-it\"  # model for generate answer\n",
        "    # answer\n",
        "\n",
        "    MYQ = \"What is in my documets base?\"\n",
        "\n",
        "    ########### PATHS ###########\n",
        "    this_project_path = os.getcwd()\n",
        "    # here you store raw documents, you shold put some files there\n",
        "    DOC_SOURCE = \"/HDD/raw_docs/\"\n",
        "\n",
        "    # following will be loaded automaticly\n",
        "    # here your models is or will be stored\n",
        "    MODEL_SOURCE = \"/HDD/models/\"\n",
        "    # here pickle with dump of your stored documents will be stored\n",
        "    DOC_LOADER_FILE = \"/HDD/data/MyDocs.pickle\"\n",
        "    # here vectorstore will be stored\n",
        "    VECTORSTORE_FILE = f\"/HDD/data/MyDocs.{VECTORSTORE2USE}{HF_EMB_MODEL.split('/')[0]}.vectorstore\"\n",
        "\n",
        "    # download models from huggingface_hub locally\n",
        "    if HF_EMB_MODEL.endswith(\".gguf\"): # if your want to use quantized model vertion\n",
        "        if not os.path.exists(os.path.join(MODEL_SOURCE, HF_EMB_MODEL)):\n",
        "            hf_hub_download(\n",
        "                repo_id=\"/\".join(HF_EMB_MODEL.split(\"/\")[:-1]),\n",
        "                filename=HF_EMB_MODEL.split(\"/\")[-1],\n",
        "                local_dir=os.path.join(MODEL_SOURCE, HF_EMB_MODEL),\n",
        "                token=HUGGINGFACEHUB_API_TOKEN,\n",
        "                force_download=True,\n",
        "            )\n",
        "    else:\n",
        "        if not os.path.exists(os.path.join(MODEL_SOURCE, HF_EMB_MODEL)):\n",
        "            snapshot_download(\n",
        "                repo_id=HF_EMB_MODEL,\n",
        "                local_dir=os.path.join(MODEL_SOURCE, HF_EMB_MODEL),\n",
        "                token=HUGGINGFACEHUB_API_TOKEN,\n",
        "                force_download=True,\n",
        "            )\n",
        "            RELOAD_VECTORSTORE = True\n",
        "    if HF_LLM_NAME.endswith(\".gguf\"): # if your want to use quantized model vertion\n",
        "        if not os.path.exists(os.path.join(MODEL_SOURCE, HF_LLM_NAME)):\n",
        "            hf_hub_download(\n",
        "                repo_id=\"/\".join(HF_LLM_NAME.split(\"/\")[:-1]),\n",
        "                filename=HF_LLM_NAME.split(\"/\")[-1],\n",
        "                local_dir=os.path.join(MODEL_SOURCE, HF_LLM_NAME),\n",
        "                token=HUGGINGFACEHUB_API_TOKEN,\n",
        "                force_download=True,\n",
        "            )\n",
        "    else:\n",
        "        if not os.path.exists(os.path.join(MODEL_SOURCE, HF_LLM_NAME)):\n",
        "            snapshot_download(\n",
        "                repo_id=HF_LLM_NAME,\n",
        "                local_dir=os.path.join(MODEL_SOURCE, HF_LLM_NAME),\n",
        "                token=HUGGINGFACEHUB_API_TOKEN,\n",
        "                force_download=True,\n",
        "            )\n",
        "\n",
        "\n",
        "# ########### LOGGING WHOLE SETUP ###########\n",
        "def print_config():\n",
        "    for i in inspect.getmembers(Config):\n",
        "        if (not i[0].startswith(\"_\")) and (not inspect.ismethod(i[1])):\n",
        "            print(f\"{i[0]} : {i[1]}\")\n",
        "\n",
        "print_config()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnPjkaNxUF32",
        "outputId": "2c75e350-0e09-4659-cad9-901e201134f6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE_EMB : cpu\n",
            "DEVICE_GEN : 1\n",
            "DOC_LOADER_FILE : /HDD/data/MyDocs.pickle\n",
            "DOC_SOURCE : /HDD/raw_docs/\n",
            "HF_EMB_MODEL : google/gemma-2b-it\n",
            "HF_LLM_NAME : google/gemma-2b-it\n",
            "MODEL_SOURCE : /HDD/models/\n",
            "MYQ : What is in my documets base?\n",
            "RELOAD_VECTORSTORE : False\n",
            "VECTORSTORE2USE : FAISS\n",
            "VECTORSTORE_FILE : /HDD/data/MyDocs.FAISSgoogle.vectorstore\n",
            "source_path : /langchain_rag_data\n",
            "this_project_path : /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GENERATOR MODEL\n",
        "Load model for generating answer"
      ],
      "metadata": {
        "id": "yNQ2ohnwWAXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "############## GENERATOR MODEL ##############\n",
        "# Load model for generating answer\n",
        "logger.info(f\"LLM : {Config.HF_LLM_NAME}\")\n",
        "# llm_gen = create_llm_gen_default()\n",
        "# OR This returns:\n",
        "llm_gen = HuggingFacePipeline.from_model_id(\n",
        "    # https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_pipeline\n",
        "    # .HuggingFacePipeline.html\n",
        "    model_id=os.path.join(Config.MODEL_SOURCE, Config.HF_LLM_NAME),\n",
        "    task=\"text-generation\",\n",
        "    # device=-1,  # -1 stands for CPU\n",
        "    device=0,  # -1 stands for CPU\n",
        "    pipeline_kwargs={\n",
        "        # full list of parameters for this section with explanation:\n",
        "        # https://huggingface.co/docs/transformers/en/main_classes/text_generation\n",
        "        # Note: some of them (depends on the specific model) should go to the model_kwargs attribute\n",
        "        \"max_new_tokens\": 512,  # How long could be generated answer\n",
        "        \"return_full_text\": False,\n",
        "        # \"return_full_text\": True if you want to return within generation answer also all prompts,\n",
        "        # contexts and other serving instrumentals\n",
        "    },\n",
        "    model_kwargs={\n",
        "        # full list of parameters for this section with explanation:\n",
        "        # https://huggingface.co/docs/transformers/en/main_classes/text_generation\n",
        "        # Note: some of them (depends on the specific model) should go to the pipeline_kwargs attribute\n",
        "        \"do_sample\": True,\n",
        "        \"top_k\": 10,\n",
        "        \"temperature\": 0.01,\n",
        "        \"repetition_penalty\": 1.03,  # 1.0 means no penalty\n",
        "        \"max_length\": 20,\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "d72d8390ca3b4f7f9a73df06dc2879eb",
            "6f8c0f672bbe4572b61ca7249a4d760a",
            "8803b4542dfb4a49afc9b8439477202d",
            "865ab1042a7f4e36aa325979e44b6a5f",
            "3cde8d76833a4f2c982c192061ff87aa",
            "e0f40e4e00834e7dbfad866ac3b2d230",
            "1e32f0f3f6664950905359a6af3c3a28",
            "d6ed4243fe854204a1dd0d41c5d34085",
            "ec02f4953837494da0fef045252b0bd8",
            "59e609526ad6478fb20ffe0ca45b78de",
            "37873e8c9c4e4d9a833824e659abe790"
          ]
        },
        "id": "ABPT7GWhyByn",
        "outputId": "3c3d4971-061d-41d1-c8a1-68429bf6c02e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-07-13 12:49:48 - INFO - LLM : google/gemma-2b-it\n",
            "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d72d8390ca3b4f7f9a73df06dc2879eb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD DOCUMENTS BASE\n",
        "Create new vectorstore (FAISS)"
      ],
      "metadata": {
        "id": "GR0jdCtQWJk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from langchain_community.document_loaders import pdf\n",
        "\n",
        "############## LOAD DOCUMENTS BASE ##############\n",
        "# Create new vectorstore (FAISS)\n",
        "logger.info(\"#### RELOAD_VECTORSTORE ####\")\n",
        "# Load Documents\n",
        "docs = []\n",
        "logger.info(\"#### LOAD RAW DOCS ####\")\n",
        "for file_name in os.listdir(Config.DOC_SOURCE):\n",
        "    fp = os.path.join(Config.DOC_SOURCE, file_name)\n",
        "\n",
        "    docs += pdf.PyPDFLoader(fp).load() # this contains list of texts from my documents base\n",
        "\n",
        "logger.info(f\"dump raw docs to {Config.DOC_LOADER_FILE} file\")\n",
        "pickle.dump(docs, open(Config.DOC_LOADER_FILE, \"wb\"))"
      ],
      "metadata": {
        "id": "uRBTXBZsWIBq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd7d5189-fde9-45db-d8d2-5abf3efd8bba"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-07-13 12:50:38 - INFO - #### RELOAD_VECTORSTORE ####\n",
            "2024-07-13 12:50:39 - INFO - #### LOAD RAW DOCS ####\n",
            "2024-07-13 12:50:41 - INFO - dump raw docs to /HDD/data/MyDocs.pickle file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEXT SPLITTER FOR DOCUMENTS\n",
        "Split documents to chunks, retriever will search through embedded chunks, not whole documents"
      ],
      "metadata": {
        "id": "OGWbZ9nmeUh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "############## TEXT SPLITTER FOR DOCUMENTS ##############\n",
        "# split documents to chunks, retriever will search through embedded chunks, not whole documents\n",
        "logger.info(\"Split\")\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=5000,  # num of characters in single chunk\n",
        "    chunk_overlap=200,  # num of characters to appear in neighborous chunks\n",
        ")\n",
        "splits = text_splitter.split_documents(docs)\n",
        "del docs  # for gc\n",
        "logger.info(f\"Num of splits : {len(splits)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5Qg46UheYYk",
        "outputId": "5687e3d7-48e5-4110-ae19-b657ec213da1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-07-13 12:50:41 - INFO - Split\n",
            "2024-07-13 12:50:41 - INFO - Num of splits : 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EMBEDDING MODEL\n",
        "Load model for embedding documents"
      ],
      "metadata": {
        "id": "sjJpBLo3VxKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "############## EMBEDDING MODEL ##############\n",
        "# Load model for embedding documents\n",
        "logger.info(f\"LLM_EMB : {Config.HF_EMB_MODEL}\")\n",
        "# llm_emb = create_llm_emb_default()\n",
        "# OR This return:\n",
        "llm_emb = HuggingFaceEmbeddings(\n",
        "    # https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.huggingface\n",
        "    # .HuggingFaceEmbeddings.html\n",
        "    model_name=os.path.join(Config.MODEL_SOURCE, Config.HF_EMB_MODEL),\n",
        "    model_kwargs={\n",
        "        # full list of parameters for this section with explanation:\n",
        "        # https://sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html\n",
        "        # #sentence_transformers.SentenceTransformer\n",
        "        \"device\": \"cpu\"\n",
        "        # \"device\": \"gpu\"\n",
        "    },\n",
        "    encode_kwargs={\n",
        "        # full list of parameters for this section with explanation:\n",
        "        # https://sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html\n",
        "        # #sentence_transformers.SentenceTransformer.encode\n",
        "        \"normalize_embeddings\": False\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "6d3dbbcda80640a5ba007addc0ad48de",
            "45f5f8e7eaa642c5a10f1cbaa81e61c8",
            "165e769fba674ec6aaf816fe2bd52865",
            "9dff6fcfd534420cb709a104f460db79",
            "901a2c1fcfc54e2d888a95e15e21ec0e",
            "571bf27c1cb74bcd89801ce0ef6253b1",
            "cab8458a5c0543969870cf3ee402b449",
            "226748a21c914a5dab07b781d333284a",
            "67e18d2e713c4bf785dedff12eb3a2e1",
            "f7f9e8e61a254d5c8b3e856ef07c52a7",
            "855dc8e6038146708771a7d492878dce"
          ]
        },
        "id": "5qCk6bT9UZju",
        "outputId": "b7f17148-68d9-436f-85d4-f8bed51be95c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-07-13 12:50:41 - INFO - LLM_EMB : google/gemma-2b-it\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
            "  warn_deprecated(\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /HDD/models/google/gemma-2b-it. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d3dbbcda80640a5ba007addc0ad48de"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VECTORSTORE FOR EMBEDDINGS\n",
        "Create vector store FAISS"
      ],
      "metadata": {
        "id": "2BUKStpbetNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from tqdm import tqdm\n",
        "# from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# ############## VECTORSTORE FOR EMBEDDINGS ##############\n",
        "# # create vector store FAISS\n",
        "# # https://python.langchain.com/v0.1/docs/integrations/vectorstores/faiss/\n",
        "# # for Num of splits : 700 will take Time : ~60min\n",
        "# logger.info(\"vectorstore FAISS\")\n",
        "# # do whole work in one approach (Note: FAISS has no verbose parameter)\n",
        "# # vectorstore = FAISS.from_documents(documents=splits,\n",
        "# #                                    embedding=llm_emb)\n",
        "# # add progress bar to FAISS creating procedure, to see some verbose:\n",
        "# vectorstore = FAISS.from_documents(\n",
        "#     documents=[splits[0]], embedding=llm_emb  # here we provide our embedding model\n",
        "# )\n",
        "# splits = splits[1:]\n",
        "# for d in tqdm(splits, desc=\"vectorstore FAISS documents\"):\n",
        "#     vectorstore.add_documents([d])\n",
        "# del splits  # for gc"
      ],
      "metadata": {
        "id": "i8QZ65hQetdQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVE and LOAD FAISS VECTORSTORE with EMBEDDING MODEL\n",
        "(Note: Chroma has another signature)\n",
        "\n",
        "- after this step we do not need `llm_emb` model anymore"
      ],
      "metadata": {
        "id": "1F2NZvAZgROo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# save vectorstore FAISS to the disk (Note: Chroma has another signature)\n",
        "# vectorstore.save_local(Config.VECTORSTORE_FILE)\n",
        "\n",
        "# load vectorstore FAISS from the disk (Note: Chroma has another signature)\n",
        "logger.info(\"vectorstore FAISS from dump\")\n",
        "vectorstore = FAISS.load_local(\n",
        "    folder_path=Config.VECTORSTORE_FILE,\n",
        "    embeddings=llm_emb,  # here we provide our embedding model\n",
        "    allow_dangerous_deserialization=True,  # True for data (docs) with loading from a pickle file.\n",
        ")\n",
        "del llm_emb # for gc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yul68MFCe-tM",
        "outputId": "28311be6-618b-4c33-f3f2-808e2fbe910b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-07-13 12:51:08 - INFO - vectorstore FAISS from dump\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RETRIEVER MODEL FROM VECTORSTORE"
      ],
      "metadata": {
        "id": "vuAgXMQ2gd4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############## RETRIEVER MODEL FROM EMBEDDING MODEL ##############\n",
        "logger.info(\"RETRIEVER\")\n",
        "retriever = vectorstore.as_retriever(\n",
        "    # full list of parameters for this section with explanation:\n",
        "    # https://api.python.langchain.com/en/latest/vectorstores/langchain_chroma.vectorstores.Chroma.html\n",
        "    # #langchain_chroma.vectorstores.Chroma.as_retriever\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\n",
        "        \"k\": 4\n",
        "    },  # return top-4 relevant (according to search_type) documents for single query\n",
        ")\n",
        "del vectorstore  # for gc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_Z3t53xgeG3",
        "outputId": "857bd8bb-315c-4052-f8a9-58a1a71f342f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-07-13 12:51:09 - INFO - RETRIEVER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V1: FULL RAG = RETRIEVER + GENERATOR\n",
        "- with **`create_stuff_documents_chain`** from `langchain.chains.combine_documents` and **`create_retrieval_chain`** from `langchain.chains.retrieval`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3wLMpSVdiNQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# some serving stuff\n",
        "def invoke_input_context_answer(chain_invoke_result):\n",
        "    answer = \"\"\n",
        "    answer += \"QUESTION: \\n\"\n",
        "    answer += chain_invoke_result.get(\"input\")\n",
        "    answer += \"\\n\\n\"\n",
        "    answer += \"BASED DOCUMENTS: \\n\"\n",
        "    for d in chain_invoke_result.get(\"context\"):\n",
        "        answer += (\n",
        "            d.metadata.get(\"source\").split(\"/\")[-1]\n",
        "            + \", page : \"\n",
        "            + str(d.metadata.get(\"page\"))\n",
        "            + \"\\n\"\n",
        "        )\n",
        "    answer += \"\\n\\n\"\n",
        "    answer += \"ANSWER: \\n\"\n",
        "    answer += (\n",
        "        chain_invoke_result.get(\"answer\").split(\"*** Helpful Answer***:\")[-1].strip()\n",
        "    )\n",
        "    return answer"
      ],
      "metadata": {
        "id": "ByPES_e3oVYp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt"
      ],
      "metadata": {
        "id": "xWIYDSIWpt6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_template_input_context = \"\"\"\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "Please follow the following rules:\n",
        "1. If you don't know the answer, don't try to make up an answer.\n",
        "Just say \"I can't find the final answer but you may want to check the following links\".\n",
        "2. If you find the answer, write the answer in a concise way with five sentences maximum.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {input}\n",
        "\n",
        "*** Helpful Answer***:\n",
        "\"\"\"\n",
        "\n",
        "# Prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=prompt_template_input_context,\n",
        "    input_variables=[\"context\", \"input\"],\n",
        ")"
      ],
      "metadata": {
        "id": "OX3qVCUfpuGI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "\n",
        "############## V1 FULL RAG = RETRIEVER + GENERATOR ##############\n",
        "logger.info(\"Classical RETRIEVER and GENERATOR\")\n",
        "question_answer_chain = create_stuff_documents_chain(llm_gen, prompt)\n",
        "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "logger.info(\"rag_chain.invoke\")\n",
        "result = chain.invoke({\"input\": Config.MYQ})\n",
        "print(invoke_input_context_answer(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et2x1WqTiNby",
        "outputId": "1f52f893-74d2-4c53-a041-22cfba96153b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-07-13 12:51:09 - INFO - Classical RETRIEVER and GENERATOR\n",
            "2024-07-13 12:51:09 - INFO - rag_chain.invoke\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1283: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QUESTION: \n",
            "What is in my documets base?\n",
            "\n",
            "BASED DOCUMENTS: \n",
            "RE FTA Decision No. 3 of 2024 on Registration Timeline for Corporate Tax  - For publishing.pdf, page : 2\n",
            "Federal Decree-Law No. 47 of 2022 - For publishing.pdf, page : 1\n",
            "RE FTA Decision No. 3 of 2024 on Registration Timeline for Corporate Tax  - For publishing.pdf, page : 0\n",
            "Federal Decree-Law No. 47 of 2022 - For publishing.pdf, page : 14\n",
            "\n",
            "\n",
            "ANSWER: \n",
            "The context does not provide information about your documents base, so I cannot answer this question from the provided context.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V2: FULL RAG = RETRIEVER + GENERATOR\n",
        "- with **`RetrievalQA.from_chain_type`** from `langchain.chains.retrieval_qa.base`"
      ],
      "metadata": {
        "id": "NiuvxqE5iUey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# some serving stuff\n",
        "def invoke_query_source_documents_result(chain_invoke_result):\n",
        "    answer = \"\"\n",
        "    answer += \"QUESTION: \\n\"\n",
        "    answer += chain_invoke_result.get(\"query\")\n",
        "    answer += \"\\n\\n\"\n",
        "    answer += \"BASED DOCUMENTS: \\n\"\n",
        "    for d in chain_invoke_result.get(\"source_documents\"):\n",
        "        answer += (\n",
        "            d.metadata.get(\"source\").split(\"/\")[-1]\n",
        "            + \", page : \"\n",
        "            + str(d.metadata.get(\"page\"))\n",
        "            + \"\\n\"\n",
        "        )\n",
        "    answer += \"\\n\\n\"\n",
        "    answer += \"ANSWER: \\n\"\n",
        "    answer += (\n",
        "        chain_invoke_result.get(\"result\").split(\"Generate according to:\")[-1].strip()\n",
        "    )\n",
        "    return answer"
      ],
      "metadata": {
        "id": "_DRAZXAZqRbr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Not need Prompt, use default"
      ],
      "metadata": {
        "id": "6OvdBN-mqIFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
        "############## V2 FULL RAG = RETRIEVER + GENERATOR ##############\n",
        "logger.info(\"Classical RETRIEVER and GENERATOR with chain type\")\n",
        "chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm_gen,\n",
        "    chain_type=\"refine\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        ")\n",
        "logger.info(\"RetrievalQA.rag_chain.invoke\")\n",
        "result = chain.invoke({\"query\": Config.MYQ})\n",
        "print(invoke_query_source_documents_result(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDNpzjeBiUwi",
        "outputId": "102bd5f7-2ed9-46e6-b91e-3d12d656a3ad"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-07-13 12:51:19 - INFO - Classical RETRIEVER and GENERATOR with chain type\n",
            "2024-07-13 12:51:19 - INFO - RetrievalQA.rag_chain.invoke\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QUESTION: \n",
            "What is in my documets base?\n",
            "\n",
            "BASED DOCUMENTS: \n",
            "RE FTA Decision No. 3 of 2024 on Registration Timeline for Corporate Tax  - For publishing.pdf, page : 2\n",
            "Federal Decree-Law No. 47 of 2022 - For publishing.pdf, page : 1\n",
            "RE FTA Decision No. 3 of 2024 on Registration Timeline for Corporate Tax  - For publishing.pdf, page : 0\n",
            "Federal Decree-Law No. 47 of 2022 - For publishing.pdf, page : 14\n",
            "\n",
            "\n",
            "ANSWER: \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V3: FULL RAG = RETRIEVER + GENERATOR\n",
        "- with **Runnable Sequences**"
      ],
      "metadata": {
        "id": "K7HINkWOikYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate multiple alternatives to the question formulation\n"
      ],
      "metadata": {
        "id": "jLTzSXI9pUdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "# some serving stuff\n",
        "def invoke_generate_queries_with_origin(input_dict: dict) -> str:\n",
        "    \"\"\"Attach Original question to generated Alternatives\"\"\"\n",
        "    # input: queries_result: dict with keys (\"question\", \"alternatives\")\n",
        "    # names of these key were setted up in the variables for calling chain\n",
        "    question = input_dict.get(\"question\")\n",
        "    alternatives = input_dict.get(\"alternatives\").replace(\"\\n\\n\", \"\\n\")\n",
        "    new_queries = f\"Original question: {question}?\" + alternatives\n",
        "    return new_queries\n",
        "\n",
        "def invoke_unique_docs_union_from_retriever(documents: list[list]) -> list:\n",
        "    \"\"\"Unique union of retrieved docs\"\"\"\n",
        "    # Flatten list of lists, and convert each Document to string\n",
        "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
        "    # Get unique documents\n",
        "    unique_docs = list(set(flattened_docs))\n",
        "    # Return\n",
        "    return [loads(doc) for doc in unique_docs]"
      ],
      "metadata": {
        "id": "HiEELH6YrJsJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt for multiple alternatives to the question formulation"
      ],
      "metadata": {
        "id": "AqXUJBtjpL8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_multi_query = \"\"\"You are an AI language model assistant. Your task is to generate {question_numbers}\n",
        "different versions of the given user question to retrieve relevant documents from a vector\n",
        "database. By generating multiple perspectives on the user question, your goal is to help\n",
        "the user overcome some of the limitations of the distance-based similarity search.\n",
        "Provide these alternative questions separated by newlines.\n",
        "\n",
        "Original question: {question}\n",
        "\"\"\"\n",
        "\n",
        "# Prompt for multiple alternatives to the question formulation\n",
        "prompt_multi_query = PromptTemplate(\n",
        "    template=prompt_multi_query,\n",
        "    # you can create any imagined prompt as template.\n",
        "    # Note: if your prompt refers to some variables in formatting type, you should provide these variables\n",
        "    # names to input_variables parameter\n",
        "    input_variables=[\"question\", \"number_questions\"],\n",
        ")"
      ],
      "metadata": {
        "id": "6jrHA85cpMLb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Chain for generating multiple alternatives to the question formulation\n",
        "generate_queries_chain = (\n",
        "    {\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "        \"question_numbers\": itemgetter(\"question_numbers\"),\n",
        "        # my prompt has a variable for number of alternative questions to generate.\n",
        "        # Actual value will be taken from this.invoke({}) calling from key \"question_numbers\"\n",
        "    }\n",
        "    | prompt_multi_query\n",
        "    | llm_gen\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "7U0oVfFFiki5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The generate_queries_chain is a pipeline built using LangChain's `RunnableSequence`. How this work?\n",
        "\n",
        "**Long story short**: output from the previous `RunnableSequence` element is passed as input to the next `RunnableSequence` element. The output type of the previous element must be compatible with the input type of the next element.\n",
        "  \n",
        "**Input Data**: The input to the chain is a `dictionary` that contains at least two keys: `question` and `question_numbers`. These values are extracted from the input dictionary using the itemgetter function.\n",
        "  \n",
        "**Prompt for Multiple Queries**: The extracted `question` and `question_numbers` are passed to the `prompt_multi_query` function. This function likely formats these inputs into a specific prompt template or prepares them for the language model (LLM).\n",
        "  \n",
        "**Language Model (LLM)**: The formatted prompt is then passed to the language model (`llm`). The LLM generates a response based on the input prompt.\n",
        "  \n",
        "**Output Parsing**: The response from the LLM is parsed using `StrOutputParser()`. This parser converts the raw output string from the LLM into a more structured format.\n",
        "  \n",
        "**Output**: The final output of the chain is the structured response from the LLM, after being parsed by `StrOutputParser()`.\n",
        "  \n",
        "This is alternative to:\n",
        "  ```\n",
        "  input_dict = {\n",
        "      \"question\": \"What is the capital of France?\",\n",
        "      \"question_numbers\": 1\n",
        "  }\n",
        "  ```\n",
        "\n",
        "**Extract Question and Question Numbers**:\n",
        "  ```\n",
        "  question = itemgetter(\"question\")(input_dict)\n",
        "  question_numbers = itemgetter(\"question_numbers\")(input_dict)\n",
        "  # Create Prompt for Multiple Queries:\n",
        "  formatted_prompt = prompt_multi_query(question, question_numbers)\n",
        "  # Generate Response Using LLM:\n",
        "  llm_response = llm(formatted_prompt)\n",
        "  # Parse the LLM Response: Output -> parsed_response\n",
        "  parsed_response = StrOutputParser()(llm_response)\n",
        "  ```"
      ],
      "metadata": {
        "id": "MvT9GINoi-9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Generate Queries Chain\n",
        "invoke_generate_queries_chain = (\n",
        "    # Here we need to pass as input to the invoke_generate_queries_with_origin 2 variables, as keys inside dict:\n",
        "    # \"alternatives\" - output from the last step of previous chain (generate_queries_chain),\n",
        "    # as well as additional var (\"question\"). Create a dict with them as input to the RunnableLambda\n",
        "    # We also pass the name for the output of previous chain (generate_queries_chain) as key of the dict\n",
        "    {\"question\": itemgetter(\"question\"), \"alternatives\": generate_queries_chain}\n",
        "    # To enable function invoke_generate_queries_with_origin to use this dict as input, it should be\n",
        "    # RunnableLambda\n",
        "    | RunnableLambda(invoke_generate_queries_with_origin)\n",
        ")\n",
        "# to check multiple generated questions:\n",
        "result = invoke_generate_queries_chain.invoke({\"question\": Config.MYQ, \"question_numbers\": 2})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5DhJc-Si_Gq",
        "outputId": "1f38519b-1979-4c0f-dbe5-0dccba911c0d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original question: What is in my documets base??\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieval Chain for multiple alternatives to the question formulation\n",
        "    \n",
        "Retriever will embed input question (as well as my previously generated alternatives) with the same `llm_emb` model as was using for vectorstore and will provide `top_k` documents similar to the question by `search_type`.\n",
        "\n",
        "Values of `top_k` and `search_type` were provided in calling `vectorstore.as_retriever()` above."
      ],
      "metadata": {
        "id": "vFmecMDYk1MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieval Chain for multiple alternatives to the question formulation\n",
        "retrieval_chain = (\n",
        "    # We can attach previous chains as input to the next chain:\n",
        "    invoke_generate_queries_chain\n",
        "    # Next step is retriever. Here we need to split str with alternative multiple queries into list to\n",
        "    # allow retriever to deal with them separatedlly and calling .map() function.\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        "    | retriever.map()\n",
        "    | invoke_unique_docs_union_from_retriever\n",
        ")\n",
        "# to check list of retrieved documents\n",
        "result = retrieval_chain.invoke({\"question\": Config.MYQ, \"question_numbers\": 2})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaB5s6Gnk1Up",
        "outputId": "119cbd7a-1c8c-4c5d-a519-fa1b014a52ce"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': '/HDD/raw_docs/RE FTA Decision No. 3 of 2024 on Registration Timeline for Corporate Tax  - For publishing.pdf', 'page': 0}, page_content='1 Federal Tax Authority Decision No. 3 of 202 4 – Unofficial translation  \\n \\n This is not an official translation : \\nThe Timeline specified  for Registration of Taxable \\nPersons for Corporate Tax  for the purposes of Federal \\nDecree -Law No. 47 of 2022 on the Taxation of \\nCorporat ions  and Business es and its amendments  \\nFederal Tax Authority  Decision  No. 3 of 202 4 – Issue d 26 February  2024 \\n(Effective 1 March 2024) \\n \\nThe Chairman  of the Board of Directors of the Federal Tax \\nAuthority has decided:  \\n- Having reviewed the Constitution;  \\n- Federal Decree -Law No. 13 of 2016 on the Establishment of the Federal Tax \\nAuthority, and its amendments ; \\n- Federal Decree -Law No. 28 of 2022 on Tax Procedures ; \\n- Federal Decree -Law No. 47 of 2022 on the Taxation of Corporations and \\nBusinesses , and its amendments ; \\n- Cabinet Decision No. 49 of 2023 on Specifying the Categories of Businesses \\nor Business Activities Conducted by a Resident or Non -Resident Natural \\nPerson that are Subject to Corporate Tax;  \\n- Cabinet Decision No. 56 of 2023 on Determination  of a Non -Resident \\nPerson’s Nexus in the State for the Purposes of Federal Decree -Law No. 47 \\nof 2022 on the Taxation of Corporations and Businesses;  \\n- Cabinet Decision No. 74 of 202 3 on the Executive Regulation of the Federal \\nDecree -Law No. 28 of 2022 on Tax  Procedures;  \\n- Cabinet Decision No. 75 of 2023 on the Administrative Penalties for \\nViolations Related to the Application of Federal Decree -Law No. 47 of 2022 \\non the Taxation of Corporations and Businesses, and its amendments;'), Document(metadata={'source': '/HDD/raw_docs/Federal Decree-Law No. 47 of 2022 - For publishing.pdf', 'page': 1}, page_content='2  \\nFederal Decree -Law No. 47 of 2022 – Unofficial translation  - Federal Decr ee-Law No. 8 of 2017 on Value Added Tax, and its amendments,  \\n- Federal Decree -Law No. 14 of 2018 on the Central Bank and Organisation of \\nFinancial Institutions and Activities, and its amendments,  \\n- Federal Decree -Law No. 15 of 2018 on the Collection of Public Revenue and \\nFunds,  \\n- Federal Decree -Law No. 26 of 2019 on Public Finance,  \\n- Federal Decree -Law No. 19 of 2020 on Trust,  \\n- Federal Decree -Law No. 31 of 2021 promulgating the Crimes and Penalties \\nLaw, and its amendments,  \\n- Federal Decree -Law No. 32 of 2021 on Commer cial Companies,  \\n- Federal Decree -Law No. 37 of 2021 on Commercial Registry,  \\n- Federal Decree -Law No. 46 of 2021 on Electronic Transactions and Trust \\nServices,  \\n- Federal Decree -Law No. 35 of 2022 promulgating the Law of Evidence in Civil \\nand Commercial Transactio ns,  \\n- Pursuant to what was presented by the Minister of Finance and approved by \\nthe Cabinet,  \\n \\nChapter One  – General provisions  \\nArticle 1  – Definitions  \\nIn the application of the provisions of this Decree -Law, the following words and \\nexpressions shall have meanings assigned against each, unless the context otherwise \\nrequires :  \\n \\nState  : United Arab Emirates.  \\nFederal Government  : The government of the United Arab Emirates.  \\nLocal Government  : Any of the governments of the Member Emirates of \\nthe Federation.  \\nMinistry  : Ministry of Finance.  \\nMinister  : Minister of Finance.  \\nAuthority  : Federal Tax Authority.  \\nCorporate Tax  : The tax imposed by this Decree -Law on juridical \\npersons and Business income.  \\nBusiness  : Any activity conducted regularly, on an ongoing and \\nindependent basis by any Person and in any location, \\nsuch as industrial, commercial, agricultural, vocational,'), Document(metadata={'source': '/HDD/raw_docs/RE FTA Decision No. 3 of 2024 on Registration Timeline for Corporate Tax  - For publishing.pdf', 'page': 4}, page_content='5 Federal Tax Authority Decision No. 3 of 202 4 – Unofficial translation  \\n \\n Categor y of natural person s Deadline  for submitting a Tax \\nRegistration  application  \\nA Resident Person who is conducting \\na Business or Business Activity during  \\nthe 2024 Gregorian  calendar  year  or \\nsubsequent years whose total \\nTurnover derived  in a  Gregorian  \\ncalendar year exceeds the threshold \\nspecified in the relevant tax legislation  31 March of the subsequent \\nGregorian  calendar year  \\nA Non-Resident Person who is \\nconducting a Business or Business \\nActivity during the 2024 Gregorian \\ncalendar year or subsequent years \\nwhose total  Turnover  derived in a \\nGregorian calendar year exceeds the \\nthreshold specified in the relevant tax \\nlegislation  (3) three months from  the date of \\nmeeting the requirements of being \\nsubject to tax \\n \\nArticle 6 – Late Registration for Corporate Tax \\nWhere Persons referred to in Articles 3, 4 and 5 of this Decision fail to submit a \\nTax Registration application as per the timelines  stated above , Administrative \\nPenalties shall be applied in accordance with Cabinet Decision No. 75 of 2023 \\nreferred to above . \\n \\nArticle 7 - Abrogation  of Conflicting Provisions  \\nAll provisions contrary to or inconsistent with the provisions of this Decision shall \\nbe abrogated.  \\n \\nArticle 8 – Publication and Application of this  Decision  \\nThis Decision shall be published in the Official Gazette and shall come into effect \\nas of  1 March  2024 .'), Document(metadata={'source': '/HDD/raw_docs/RE FTA Decision No. 3 of 2024 on Registration Timeline for Corporate Tax  - For publishing.pdf', 'page': 2}, page_content='3 Federal Tax Authority Decision No. 3 of 202 4 – Unofficial translation  \\n \\n Date  of Licence  issuance  irrespective \\nof year of issuance  Deadline  for submitting a Tax \\nRegistration  application  \\n1 January – 31 January  \\n 31 May 2024  \\n1 February – 28/29 February  \\n 31 May 2024  \\n1 March – 31 March  30 June 2024  \\n1 April – 30 April  30 June 2024  \\n1 May – 31 May  31 July 2024  \\n1 June – 30 June  31 August 2024  \\n1 July – 31 July  30 September 2024  \\n1 August – 31 August  31 October 2024  \\n1 September – 30 September  \\n 31 October 2024  \\n1 October – 31 October  \\n 30 November 2024  \\n1 November – 30 November  \\n 30 November  2024  \\n1 December – 31 December  31 December  2024  \\nWhere a person does not have a  \\nLicence  at the effective date of this \\nDecision  (3) three  months from  the effective \\ndate of this Decision  \\n2. For the purposes of Clause 1 of this Article, w here  a juridical person has more \\nthan one Licence, the  Licence with the earliest issuance date shall be used . \\n3. A juridical person  that is a Resident  Person incorporated  or otherwise \\nestablished or recognised on or after the effective date  of this Decision, shall  \\nsubmit a Tax Registration  application , in accordance with the following table:  \\nCategor y of juridical person s Deadline  for submitting a Tax \\nRegistration  application  \\nA person that is incorporated or \\notherwise established or recognised  \\nunder the applicable legislation in the \\nState,  including a Free Zone Person  (3) three months from the date of \\nincorporation , establishment or \\nrecognition  \\nA person that is incorporated or \\notherwise established or recognised  \\nunder the applicable legislation of a (3) three  months from the end of the \\nFinancial Year of the person')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
            "  warn_beta(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt for generation answer with retriever and generation prompt"
      ],
      "metadata": {
        "id": "H5EaPovVlqtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Prompt for generation answer with retriever and generation prompt\n",
        "prompt_template_question_context = \"\"\"\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "Please follow the following rules:\n",
        "1. If you don't know the answer, don't try to make up an answer.\n",
        "Just say \"I can't find the final answer but you may want to check the following links\".\n",
        "2. If you find the answer, write the answer in a concise way with five sentences maximum.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "*** Helpful Answer***:\n",
        "\"\"\"\n",
        "\n",
        "prompt_generation = PromptTemplate(\n",
        "    template=prompt_template_question_context,\n",
        "    input_variables=[\"question\", \"context\"],\n",
        ")"
      ],
      "metadata": {
        "id": "8I6Q4pVGlq2I"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Chain\n",
        "-  Generator (could be another model as for retriever) takes list of retrieved (relevant) documents and generate answer for the qustion according to them."
      ],
      "metadata": {
        "id": "vcI0DIKIlu9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG Chain\n",
        "# Generator (could be another model as for retriever) takes list of retrieved (relevant) documents and generate\n",
        "# answer for the qustion according to them.\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": retrieval_chain,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    # Here again: since prompt_generation takes as input 2 variables with names: context and question,\n",
        "    # we assign these name to the variables as dict keys.\n",
        "    # \"context\" will take value from the output of retrieval_chain\n",
        "    # \"question\" will take value from calling this.invoke() with provided \"question\" key\n",
        "    | prompt_generation\n",
        "    | llm_gen\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke({\"question\": Config.MYQ, \"question_numbers\": 2})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnbXEwyIlx8S",
        "outputId": "2fb7d0e1-025e-49b2-a5a0-07219e3c5b3a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The document base contains a Federal Decree -Law No. 47 of 2022 on Tax Procedures, which is related to the Taxation of corporations and businesses in the United Arab Emirates.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Congratulations! You are full now!\n",
        "### Further investigations:\n",
        "\n",
        "[Advanced option to rule RAG](https://github.com/eericheva/langchain_rag?tab=readme-ov-file#item-one)"
      ],
      "metadata": {
        "id": "LZ3GlMS1xIx9"
      }
    }
  ]
}