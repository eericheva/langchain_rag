{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsPKAwZ2BiX1LnwHo+fVT1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ad4e5040d2ee445eb6bfcbdcf2417c91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4548c554b254c2fa10b32314a8227f5",
              "IPY_MODEL_ab7983bf24f64dcba45fcd10a9cbeff5",
              "IPY_MODEL_b7d2132a09704c2ea0aaa0b9a8f259b8"
            ],
            "layout": "IPY_MODEL_3df9193f931646dda355c7c0ee1a22ca"
          }
        },
        "d4548c554b254c2fa10b32314a8227f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3ac5af81cb748098aa38dd548af11fa",
            "placeholder": "​",
            "style": "IPY_MODEL_d44852261ee3448ea7d87b00e8856d25",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ab7983bf24f64dcba45fcd10a9cbeff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35f2bcb17de2481a8e295dca99e61ab4",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4dd2ed2075c349e58271adaf52ff4502",
            "value": 2
          }
        },
        "b7d2132a09704c2ea0aaa0b9a8f259b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7da8c9eec1f049d2bb1cd45266447749",
            "placeholder": "​",
            "style": "IPY_MODEL_bb00db3d35bd45bba85a930bcb659130",
            "value": " 2/2 [00:23&lt;00:00,  9.67s/it]"
          }
        },
        "3df9193f931646dda355c7c0ee1a22ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3ac5af81cb748098aa38dd548af11fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d44852261ee3448ea7d87b00e8856d25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35f2bcb17de2481a8e295dca99e61ab4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dd2ed2075c349e58271adaf52ff4502": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7da8c9eec1f049d2bb1cd45266447749": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb00db3d35bd45bba85a930bcb659130": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eericheva/langchain_rag/blob/dev/tutorials/start_here.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic and simple first RAG tutorial\n",
        "- *the same but in as python script format: [`start_here.py`](https://github.com/eericheva/langchain_rag/blob/main/tutorials/start_here.py)*\n",
        "- V1: FULL RAG = RETRIEVER + GENERATOR with **`create_stuff_documents_chain`** from `langchain.chains.combine_documents` and **`create_retrieval_chain`** from `langchain.chains.retrieval`\n",
        "- V2: FULL RAG = RETRIEVER + GENERATOR with **`RetrievalQA.from_chain_type`** from `langchain.chains.retrieval_qa.base`\n",
        "- V3: FULL RAG = RETRIEVER + GENERATOR with **Runnable Sequences**\n",
        "\n",
        "*Full repo with RAG hints and scripts [eericheva/langchain_rag](https://github.com/eericheva/langchain_rag/tree/main)*"
      ],
      "metadata": {
        "id": "-pA8RG9pfLfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community langchain_core\n",
        "!pip install huggingface_hub\n",
        "!pip install sentence-transformers\n",
        "!pip install pypdf\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2bwwDf0Qps8",
        "outputId": "85f7bb2a-6d96-4e7e-d312-6eb462c299a8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.10/dist-packages (0.2.17)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.85)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (24.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.5.82)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n",
        "# shutil.rmtree('/HDD/models/HuggingFaceH4')\n",
        "# shutil.rmtree('/HDD/models/intfloat')\n",
        "# os.makedirs(\"/langchain_rag_data/raw_docs\")\n",
        "\n",
        "# import os\n",
        "# from tqdm import tqdm\n",
        "# from operator import itemgetter\n",
        "# import logging\n",
        "# from google.colab import userdata\n",
        "# import inspect"
      ],
      "metadata": {
        "id": "iLsmBKT6YSPh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "f6w69Cx3QbZV"
      },
      "outputs": [],
      "source": [
        "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "# from langchain.chains.retrieval import create_retrieval_chain\n",
        "# from langchain.chains.retrieval_qa.base import RetrievalQA\n",
        "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "# from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "# from langchain_community.vectorstores import FAISS\n",
        "# from langchain_community.document_loaders import pdf\n",
        "# from langchain_core.output_parsers import StrOutputParser\n",
        "# from langchain_core.prompts import PromptTemplate\n",
        "# from langchain_core.runnables import RunnableLambda\n",
        "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# from huggingface_hub import hf_hub_download, snapshot_download"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "########### LOGER ###########\n",
        "logger = logging.getLogger(\"langchain_rag\")\n",
        "logger.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter(\n",
        "    fmt=\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        ")\n",
        "handler = logging.StreamHandler()\n",
        "handler.setFormatter(formatter)\n",
        "logger.handlers.clear()  # to avoid doubling in logger output\n",
        "logger.addHandler(handler)\n",
        "logger.propagate = False  # to avoid doubling in logger output"
      ],
      "metadata": {
        "id": "2bsIJ2fRUNr1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INITIAL SETUP\n",
        "\n",
        "Setup `Config` with your tokens, key and setup params: vectorstore type models, local paths, models etc"
      ],
      "metadata": {
        "id": "fBPial8cRO1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "########### KEYS AND TOKENS ###########\n",
        "# (Optional) LangSmith for closely monitor and evaluate your application. https://docs.smith.langchain.com/\n",
        "# go to the https://smith.langchain.com/settings, and create your oun LANGCHAIN_API_KEY\n",
        "LANGCHAIN_API_KEY = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "# (Optional) If you want to use OpenAI models,\n",
        "# go to the https://platform.openai.com/api-keys, and create your oun OPENAI_API_KEY\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "# (Optional) If you want to use HuggingFaceHub:\n",
        "# go to the https://huggingface.co/settings/tokens, and create your oun HUGGINGFACEHUB_API_TOKEN\n",
        "HUGGINGFACEHUB_API_TOKEN = userdata.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
      ],
      "metadata": {
        "id": "2jnlK-7WRIY-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "import os\n",
        "\n",
        "from huggingface_hub import hf_hub_download, snapshot_download\n",
        "\n",
        "class Config:\n",
        "    ########### SETUP ###########\n",
        "    source_path = \"/langchain_rag_data\"\n",
        "    RELOAD_VECTORSTORE = False  # True if you want to recreate new vector store with new embedding or new documents\n",
        "    # False, if you want to restore vectorstore from dump\n",
        "    DEVICE_EMB = \"cpu\"  # \"cpu\" stands for cpu, \"cuda:1\"\n",
        "    DEVICE_GEN = 1  # -1 stands for cpu\n",
        "\n",
        "    VECTORSTORE2USE = \"FAISS\"  # \"FAISS\", \"CHROMA\"\n",
        "    # models = repo names from hugginface_hub\n",
        "    HF_EMB_MODEL = \"google/gemma-2b-it\"  # model for embedding documents\n",
        "    HF_LLM_NAME = \"google/gemma-2b-it\"  # model for generate answer\n",
        "    # answer\n",
        "\n",
        "    MYQ = \"What is in my documets base?\"\n",
        "\n",
        "    ########### PATHS ###########\n",
        "    this_project_path = os.getcwd()\n",
        "    # here you store raw documents, you shold put some files there\n",
        "    DOC_SOURCE = os.path.join(this_project_path, source_path, \"raw_docs/\")\n",
        "\n",
        "    # following will be loaded automaticly\n",
        "    # here your models is or will be stored\n",
        "    MODEL_SOURCE = \"/HDD/models/\"\n",
        "    # here pickle with dump of your stored documents will be stored\n",
        "    DOC_LOADER_FILE = os.path.join(this_project_path, source_path, \"data/MyDocs.pickle\")\n",
        "    # here vectorstore will be stored\n",
        "    VECTORSTORE_FILE = os.path.join(\n",
        "        this_project_path,\n",
        "        source_path,\n",
        "        f\"data/MyDocs.{VECTORSTORE2USE}{HF_EMB_MODEL.split('/')[0]}.vectorstore\",\n",
        "    )\n",
        "\n",
        "    # download models from huggingface_hub locally\n",
        "    if HF_EMB_MODEL.endswith(\".gguf\"): # if your want to use quantized model vertion\n",
        "        if not os.path.exists(os.path.join(MODEL_SOURCE, HF_EMB_MODEL)):\n",
        "            hf_hub_download(\n",
        "                repo_id=\"/\".join(HF_EMB_MODEL.split(\"/\")[:-1]),\n",
        "                filename=HF_EMB_MODEL.split(\"/\")[-1],\n",
        "                local_dir=os.path.join(MODEL_SOURCE, HF_EMB_MODEL),\n",
        "                token=HUGGINGFACEHUB_API_TOKEN,\n",
        "                force_download=True,\n",
        "            )\n",
        "    else:\n",
        "        if not os.path.exists(os.path.join(MODEL_SOURCE, HF_EMB_MODEL)):\n",
        "            snapshot_download(\n",
        "                repo_id=HF_EMB_MODEL,\n",
        "                local_dir=os.path.join(MODEL_SOURCE, HF_EMB_MODEL),\n",
        "                token=HUGGINGFACEHUB_API_TOKEN,\n",
        "                force_download=True,\n",
        "            )\n",
        "            RELOAD_VECTORSTORE = True\n",
        "    if HF_LLM_NAME.endswith(\".gguf\"): # if your want to use quantized model vertion\n",
        "        if not os.path.exists(os.path.join(MODEL_SOURCE, HF_LLM_NAME)):\n",
        "            hf_hub_download(\n",
        "                repo_id=\"/\".join(HF_LLM_NAME.split(\"/\")[:-1]),\n",
        "                filename=HF_LLM_NAME.split(\"/\")[-1],\n",
        "                local_dir=os.path.join(MODEL_SOURCE, HF_LLM_NAME),\n",
        "                token=HUGGINGFACEHUB_API_TOKEN,\n",
        "                force_download=True,\n",
        "            )\n",
        "    else:\n",
        "        if not os.path.exists(os.path.join(MODEL_SOURCE, HF_LLM_NAME)):\n",
        "            snapshot_download(\n",
        "                repo_id=HF_LLM_NAME,\n",
        "                local_dir=os.path.join(MODEL_SOURCE, HF_LLM_NAME),\n",
        "                token=HUGGINGFACEHUB_API_TOKEN,\n",
        "                force_download=True,\n",
        "            )\n",
        "\n",
        "\n",
        "# ########### LOGGING WHOLE SETUP ###########\n",
        "def print_config():\n",
        "    for i in inspect.getmembers(Config):\n",
        "        if (not i[0].startswith(\"_\")) and (not inspect.ismethod(i[1])):\n",
        "            print(f\"{i[0]} : {i[1]}\")\n",
        "\n",
        "print_config()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnPjkaNxUF32",
        "outputId": "0c609e7d-a098-44f0-bcac-af0616f65059"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE_EMB : cpu\n",
            "DEVICE_GEN : 1\n",
            "DOC_LOADER_FILE : /langchain_rag_data/data/MyDocs.pickle\n",
            "DOC_SOURCE : /langchain_rag_data/raw_docs/\n",
            "HF_EMB_MODEL : google/gemma-2b-it\n",
            "HF_LLM_NAME : google/gemma-2b-it\n",
            "MODEL_SOURCE : /HDD/models/\n",
            "MYQ : What is in my documets base?\n",
            "RELOAD_VECTORSTORE : False\n",
            "VECTORSTORE2USE : FAISS\n",
            "VECTORSTORE_FILE : /langchain_rag_data/data/MyDocs.FAISSgoogle.vectorstore\n",
            "source_path : /langchain_rag_data\n",
            "this_project_path : /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EMBEDDING MODEL\n",
        "Load model for embedding documents"
      ],
      "metadata": {
        "id": "sjJpBLo3VxKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "############## EMBEDDING MODEL ##############\n",
        "# Load model for embedding documents\n",
        "logger.info(f\"LLM_EMB : {Config.HF_EMB_MODEL}\")\n",
        "# llm_emb = create_llm_emb_default()\n",
        "# OR This return:\n",
        "llm_emb = HuggingFaceEmbeddings(\n",
        "    # https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.huggingface\n",
        "    # .HuggingFaceEmbeddings.html\n",
        "    model_name=os.path.join(Config.MODEL_SOURCE, Config.HF_EMB_MODEL),\n",
        "    model_kwargs={\n",
        "        # full list of parameters for this section with explanation:\n",
        "        # https://sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html\n",
        "        # #sentence_transformers.SentenceTransformer\n",
        "        \"device\": \"cpu\"\n",
        "    },\n",
        "    encode_kwargs={\n",
        "        # full list of parameters for this section with explanation:\n",
        "        # https://sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html\n",
        "        # #sentence_transformers.SentenceTransformer.encode\n",
        "        \"normalize_embeddings\": False\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "ad4e5040d2ee445eb6bfcbdcf2417c91",
            "d4548c554b254c2fa10b32314a8227f5",
            "ab7983bf24f64dcba45fcd10a9cbeff5",
            "b7d2132a09704c2ea0aaa0b9a8f259b8",
            "3df9193f931646dda355c7c0ee1a22ca",
            "c3ac5af81cb748098aa38dd548af11fa",
            "d44852261ee3448ea7d87b00e8856d25",
            "35f2bcb17de2481a8e295dca99e61ab4",
            "4dd2ed2075c349e58271adaf52ff4502",
            "7da8c9eec1f049d2bb1cd45266447749",
            "bb00db3d35bd45bba85a930bcb659130"
          ]
        },
        "id": "5qCk6bT9UZju",
        "outputId": "5fa04d83-1a3a-40a8-a115-f5b6ffe8d789"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-07-13 10:24:51 - INFO - LLM_EMB : google/gemma-2b-it\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
            "  warn_deprecated(\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /HDD/models/google/gemma-2b-it. Creating a new one with mean pooling.\n",
            "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad4e5040d2ee445eb6bfcbdcf2417c91"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GENERATOR MODEL\n",
        "Load model for generating answer"
      ],
      "metadata": {
        "id": "yNQ2ohnwWAXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_gen = llm_emb\n",
        "# from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "# ############## GENERATOR MODEL ##############\n",
        "# # Load model for generating answer\n",
        "# logger.info(f\"LLM : {Config.HF_LLM_NAME}\")\n",
        "# # llm_gen = create_llm_gen_default()\n",
        "# # OR This returns:\n",
        "# llm_gen = HuggingFacePipeline.from_model_id(\n",
        "#     # https://api.python.langchain.com/en/latest/llms/langchain_community.llms.huggingface_pipeline\n",
        "#     # .HuggingFacePipeline.html\n",
        "#     model_id=os.path.join(Config.MODEL_SOURCE, Config.HF_LLM_NAME),\n",
        "#     task=\"text-generation\",\n",
        "#     device=-1,  # -1 stands for CPU\n",
        "#     pipeline_kwargs={\n",
        "#         # full list of parameters for this section with explanation:\n",
        "#         # https://huggingface.co/docs/transformers/en/main_classes/text_generation\n",
        "#         # Note: some of them (depends on the specific model) should go to the model_kwargs attribute\n",
        "#         \"max_new_tokens\": 512,  # How long could be generated answer\n",
        "#         \"return_full_text\": False,\n",
        "#         # \"return_full_text\": True if you want to return within generation answer also all prompts,\n",
        "#         # contexts and other serving instrumentals\n",
        "#     },\n",
        "#     model_kwargs={\n",
        "#         # full list of parameters for this section with explanation:\n",
        "#         # https://huggingface.co/docs/transformers/en/main_classes/text_generation\n",
        "#         # Note: some of them (depends on the specific model) should go to the pipeline_kwargs attribute\n",
        "#         \"do_sample\": True,\n",
        "#         \"top_k\": 10,\n",
        "#         \"temperature\": 0.0,\n",
        "#         \"repetition_penalty\": 1.03,  # 1.0 means no penalty\n",
        "#         \"max_length\": 20,\n",
        "#     },\n",
        "# )\n"
      ],
      "metadata": {
        "id": "KhecAHyuV5hD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD DOCUMENTS BASE\n",
        "Create new vectorstore (FAISS)"
      ],
      "metadata": {
        "id": "GR0jdCtQWJk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from langchain_community.document_loaders import pdf\n",
        "\n",
        "############## LOAD DOCUMENTS BASE ##############\n",
        "# Create new vectorstore (FAISS)\n",
        "logger.info(\"#### RELOAD_VECTORSTORE ####\")\n",
        "# Load Documents\n",
        "docs = []\n",
        "logger.info(\"#### LOAD RAW DOCS ####\")\n",
        "for file_name in os.listdir(Config.DOC_SOURCE):\n",
        "    fp = os.path.join(Config.DOC_SOURCE, file_name)\n",
        "\n",
        "    docs += pdf.PyPDFLoader(fp).load() # this contains list of texts from my documents base\n",
        "\n",
        "logger.info(f\"dump raw docs to {Config.DOC_LOADER_FILE} file\")\n",
        "pickle.dump(docs, open(Config.DOC_LOADER_FILE, \"wb\"))"
      ],
      "metadata": {
        "id": "uRBTXBZsWIBq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b87cb9-62fb-4ba3-de01-e52a87d515ae"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-07-13 10:25:36 - INFO - #### RELOAD_VECTORSTORE ####\n",
            "2024-07-13 10:25:36 - INFO - #### LOAD RAW DOCS ####\n",
            "2024-07-13 10:25:38 - INFO - dump raw docs to /langchain_rag_data/data/MyDocs.pickle file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEXT SPLITTER FOR DOCUMENTS\n",
        "Split documents to chunks, retriever will search through embedded chunks, not whole documents"
      ],
      "metadata": {
        "id": "OGWbZ9nmeUh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "############## TEXT SPLITTER FOR DOCUMENTS ##############\n",
        "# split documents to chunks, retriever will search through embedded chunks, not whole documents\n",
        "logger.info(\"Split\")\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=5000,  # num of characters in single chunk\n",
        "    chunk_overlap=200,  # num of characters to appear in neighborous chunks\n",
        ")\n",
        "splits = text_splitter.split_documents(docs)\n",
        "del docs  # for gc\n",
        "logger.info(f\"Num of splits : {len(splits)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5Qg46UheYYk",
        "outputId": "c32f86fc-b368-4629-d2cb-743ff30a879e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-07-13 10:25:38 - INFO - Split\n",
            "2024-07-13 10:25:39 - INFO - Num of splits : 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VECTORSTORE FOR EMBEDDINGS\n",
        "Create vector store FAISS"
      ],
      "metadata": {
        "id": "2BUKStpbetNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "############## VECTORSTORE FOR EMBEDDINGS ##############\n",
        "# create vector store FAISS\n",
        "# https://python.langchain.com/v0.1/docs/integrations/vectorstores/faiss/\n",
        "# for Num of splits : 700 will take Time : ~60min\n",
        "logger.info(\"vectorstore FAISS\")\n",
        "# do whole work in one approach (Note: FAISS has no verbose parameter)\n",
        "# vectorstore = FAISS.from_documents(documents=splits,\n",
        "#                                    embedding=llm_emb)\n",
        "# add progress bar to FAISS creating procedure, to see some verbose:\n",
        "vectorstore = FAISS.from_documents(\n",
        "    documents=[splits[0]], embedding=llm_emb  # here we provide our embedding model\n",
        ")\n",
        "splits = splits[1:]\n",
        "for d in tqdm(splits, desc=\"vectorstore FAISS documents\"):\n",
        "    vectorstore.add_documents([d])\n",
        "del splits  # for gc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8QZ65hQetdQ",
        "outputId": "c54fe596-5dfe-4654-99d5-fa6e5068efe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-07-13 10:25:39 - INFO - vectorstore FAISS\n",
            "vectorstore FAISS documents:  55%|█████▍    | 35/64 [19:28<17:11, 35.58s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVE and LOAD FAISS VECTORSTORE\n",
        "(Note: Chroma has another signature)"
      ],
      "metadata": {
        "id": "1F2NZvAZgROo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save vectorstore FAISS to the disk (Note: Chroma has another signature)\n",
        "vectorstore.save_local(Config.VECTORSTORE_FILE)\n",
        "\n",
        "# load vectorstore FAISS from the disk (Note: Chroma has another signature)\n",
        "logger.info(\"vectorstore FAISS from dump\")\n",
        "vectorstore = FAISS.load_local(\n",
        "    folder_path=Config.VECTORSTORE_FILE,\n",
        "    embeddings=llm_emb,  # here we provide our embedding model\n",
        "    allow_dangerous_deserialization=True,  # True for data (docs) with loading from a pickle file.\n",
        ")"
      ],
      "metadata": {
        "id": "yul68MFCe-tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RETRIEVER MODEL FROM EMBEDDING MODEL"
      ],
      "metadata": {
        "id": "vuAgXMQ2gd4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############## RETRIEVER MODEL FROM EMBEDDING MODEL ##############\n",
        "logger.info(\"RETRIEVER\")\n",
        "retriever = vectorstore.as_retriever(\n",
        "    # full list of parameters for this section with explanation:\n",
        "    # https://api.python.langchain.com/en/latest/vectorstores/langchain_chroma.vectorstores.Chroma.html\n",
        "    # #langchain_chroma.vectorstores.Chroma.as_retriever\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\n",
        "        \"k\": 4\n",
        "    },  # return top-4 relevant (according to search_type) documents for single query\n",
        ")\n",
        "del vectorstore  # for gc"
      ],
      "metadata": {
        "id": "a_Z3t53xgeG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V1: FULL RAG = RETRIEVER + GENERATOR\n",
        "- with **`create_stuff_documents_chain`** from `langchain.chains.combine_documents` and **`create_retrieval_chain`** from `langchain.chains.retrieval`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3wLMpSVdiNQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# some serving stuff\n",
        "def invoke_input_context_answer(chain_invoke_result):\n",
        "    answer = \"\"\n",
        "    answer += \"QUESTION: \\n\"\n",
        "    answer += chain_invoke_result.get(\"input\")\n",
        "    answer += \"\\n\\n\"\n",
        "    answer += \"BASED DOCUMENTS: \\n\"\n",
        "    for d in chain_invoke_result.get(\"context\"):\n",
        "        answer += (\n",
        "            d.metadata.get(\"source\").split(\"/\")[-1]\n",
        "            + \", page : \"\n",
        "            + str(d.metadata.get(\"page\"))\n",
        "            + \"\\n\"\n",
        "        )\n",
        "    answer += \"\\n\\n\"\n",
        "    answer += \"ANSWER: \\n\"\n",
        "    answer += (\n",
        "        chain_invoke_result.get(\"answer\").split(\"*** Helpful Answer***:\")[-1].strip()\n",
        "    )\n",
        "    return answer"
      ],
      "metadata": {
        "id": "ByPES_e3oVYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt"
      ],
      "metadata": {
        "id": "xWIYDSIWpt6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_template_input_context = \"\"\"\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "Please follow the following rules:\n",
        "1. If you don't know the answer, don't try to make up an answer.\n",
        "Just say \"I can't find the final answer but you may want to check the following links\".\n",
        "2. If you find the answer, write the answer in a concise way with five sentences maximum.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {input}\n",
        "\n",
        "*** Helpful Answer***:\n",
        "\"\"\"\n",
        "\n",
        "# Prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=prompt_template_input_context,\n",
        "    input_variables=[\"context\", \"input\"],\n",
        ")"
      ],
      "metadata": {
        "id": "OX3qVCUfpuGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "\n",
        "############## V1 FULL RAG = RETRIEVER + GENERATOR ##############\n",
        "logger.info(\"Classical RETRIEVER and GENERATOR\")\n",
        "question_answer_chain = create_stuff_documents_chain(llm_gen, prompt)\n",
        "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "logger.info(\"rag_chain.invoke\")\n",
        "result = chain.invoke({\"input\": Config.MYQ})\n",
        "print(invoke_input_context_answer(result))"
      ],
      "metadata": {
        "id": "Et2x1WqTiNby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V2: FULL RAG = RETRIEVER + GENERATOR\n",
        "- with **`RetrievalQA.from_chain_type`** from `langchain.chains.retrieval_qa.base`"
      ],
      "metadata": {
        "id": "NiuvxqE5iUey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# some serving stuff\n",
        "def invoke_query_source_documents_result(chain_invoke_result):\n",
        "    answer = \"\"\n",
        "    answer += \"QUESTION: \\n\"\n",
        "    answer += chain_invoke_result.get(\"query\")\n",
        "    answer += \"\\n\\n\"\n",
        "    answer += \"BASED DOCUMENTS: \\n\"\n",
        "    for d in chain_invoke_result.get(\"source_documents\"):\n",
        "        answer += (\n",
        "            d.metadata.get(\"source\").split(\"/\")[-1]\n",
        "            + \", page : \"\n",
        "            + str(d.metadata.get(\"page\"))\n",
        "            + \"\\n\"\n",
        "        )\n",
        "    answer += \"\\n\\n\"\n",
        "    answer += \"ANSWER: \\n\"\n",
        "    answer += (\n",
        "        chain_invoke_result.get(\"result\").split(\"Generate according to:\")[-1].strip()\n",
        "    )\n",
        "    return answer"
      ],
      "metadata": {
        "id": "_DRAZXAZqRbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Not need Prompt, use default"
      ],
      "metadata": {
        "id": "6OvdBN-mqIFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
        "############## V2 FULL RAG = RETRIEVER + GENERATOR ##############\n",
        "logger.info(\"Classical RETRIEVER and GENERATOR with chain type\")\n",
        "chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm_gen,\n",
        "    chain_type=\"refine\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        ")\n",
        "logger.info(\"RetrievalQA.rag_chain.invoke\")\n",
        "result = chain.invoke({\"query\": Config.MYQ})\n",
        "print(invoke_query_source_documents_result(result))"
      ],
      "metadata": {
        "id": "QDNpzjeBiUwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V3: FULL RAG = RETRIEVER + GENERATOR\n",
        "- with **Runnable Sequences**"
      ],
      "metadata": {
        "id": "K7HINkWOikYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate multiple alternatives to the question formulation\n"
      ],
      "metadata": {
        "id": "jLTzSXI9pUdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "# some serving stuff\n",
        "def invoke_generate_queries_with_origin(input_dict: dict) -> str:\n",
        "    \"\"\"Attach Original question to generated Alternatives\"\"\"\n",
        "    # input: queries_result: dict with keys (\"question\", \"alternatives\")\n",
        "    # names of these key were setted up in the variables for calling chain\n",
        "    question = input_dict.get(\"question\")\n",
        "    alternatives = input_dict.get(\"alternatives\").replace(\"\\n\\n\", \"\\n\")\n",
        "    new_queries = f\"Original question: {question}?\" + alternatives\n",
        "    return new_queries\n",
        "\n",
        "def invoke_unique_docs_union_from_retriever(documents: list[list]) -> list:\n",
        "    \"\"\"Unique union of retrieved docs\"\"\"\n",
        "    # Flatten list of lists, and convert each Document to string\n",
        "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
        "    # Get unique documents\n",
        "    unique_docs = list(set(flattened_docs))\n",
        "    # Return\n",
        "    return [loads(doc) for doc in unique_docs]"
      ],
      "metadata": {
        "id": "HiEELH6YrJsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt for multiple alternatives to the question formulation"
      ],
      "metadata": {
        "id": "AqXUJBtjpL8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_multi_query = \"\"\"You are an AI language model assistant. Your task is to generate {question_numbers}\n",
        "different versions of the given user question to retrieve relevant documents from a vector\n",
        "database. By generating multiple perspectives on the user question, your goal is to help\n",
        "the user overcome some of the limitations of the distance-based similarity search.\n",
        "Provide these alternative questions separated by newlines.\n",
        "\n",
        "Original question: {question}\n",
        "\"\"\"\n",
        "\n",
        "# Prompt for multiple alternatives to the question formulation\n",
        "prompt_multi_query = PromptTemplate(\n",
        "    template=prompt_multi_query,\n",
        "    # you can create any imagined prompt as template.\n",
        "    # Note: if your prompt refers to some variables in formatting type, you should provide these variables\n",
        "    # names to input_variables parameter\n",
        "    input_variables=[\"question\", \"number_questions\"],\n",
        ")"
      ],
      "metadata": {
        "id": "6jrHA85cpMLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Chain for generating multiple alternatives to the question formulation\n",
        "generate_queries_chain = (\n",
        "    {\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "        \"question_numbers\": itemgetter(\"question_numbers\"),\n",
        "        # my prompt has a variable for number of alternative questions to generate.\n",
        "        # Actual value will be taken from this.invoke({}) calling from key \"question_numbers\"\n",
        "    }\n",
        "    | prompt_multi_query\n",
        "    | llm_gen\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "7U0oVfFFiki5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The generate_queries_chain is a pipeline built using LangChain's `RunnableSequence`. How this work?\n",
        "\n",
        "**Long story short**: output from the previous `RunnableSequence` element is passed as input to the next `RunnableSequence` element. The output type of the previous element must be compatible with the input type of the next element.\n",
        "  \n",
        "**Input Data**: The input to the chain is a `dictionary` that contains at least two keys: `question` and `question_numbers`. These values are extracted from the input dictionary using the itemgetter function.\n",
        "  \n",
        "**Prompt for Multiple Queries**: The extracted `question` and `question_numbers` are passed to the `prompt_multi_query` function. This function likely formats these inputs into a specific prompt template or prepares them for the language model (LLM).\n",
        "  \n",
        "**Language Model (LLM)**: The formatted prompt is then passed to the language model (`llm`). The LLM generates a response based on the input prompt.\n",
        "  \n",
        "**Output Parsing**: The response from the LLM is parsed using `StrOutputParser()`. This parser converts the raw output string from the LLM into a more structured format.\n",
        "  \n",
        "**Output**: The final output of the chain is the structured response from the LLM, after being parsed by `StrOutputParser()`.\n",
        "  \n",
        "This is alternative to:\n",
        "  ```\n",
        "  input_dict = {\n",
        "      \"question\": \"What is the capital of France?\",\n",
        "      \"question_numbers\": 1\n",
        "  }\n",
        "  ```\n",
        "\n",
        "**Extract Question and Question Numbers**:\n",
        "  ```\n",
        "  question = itemgetter(\"question\")(input_dict)\n",
        "  question_numbers = itemgetter(\"question_numbers\")(input_dict)\n",
        "  # Create Prompt for Multiple Queries:\n",
        "  formatted_prompt = prompt_multi_query(question, question_numbers)\n",
        "  # Generate Response Using LLM:\n",
        "  llm_response = llm(formatted_prompt)\n",
        "  # Parse the LLM Response: Output -> parsed_response\n",
        "  parsed_response = StrOutputParser()(llm_response)\n",
        "  ```"
      ],
      "metadata": {
        "id": "MvT9GINoi-9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Generate Queries Chain\n",
        "invoke_generate_queries_chain = (\n",
        "    # Here we need to pass as input to the invoke_generate_queries_with_origin 2 variables, as keys inside dict:\n",
        "    # \"alternatives\" - output from the last step of previous chain (generate_queries_chain),\n",
        "    # as well as additional var (\"question\"). Create a dict with them as input to the RunnableLambda\n",
        "    # We also pass the name for the output of previous chain (generate_queries_chain) as key of the dict\n",
        "    {\"question\": itemgetter(\"question\"), \"alternatives\": generate_queries_chain}\n",
        "    # To enable function invoke_generate_queries_with_origin to use this dict as input, it should be\n",
        "    # RunnableLambda\n",
        "    | RunnableLambda(invoke_generate_queries_with_origin)\n",
        ")\n",
        "# to check multiple generated questions:\n",
        "result = invoke_generate_queries_chain.invoke({\"question\": Config.MYQ, \"question_numbers\": 2})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "-5DhJc-Si_Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieval Chain for multiple alternatives to the question formulation\n",
        "    \n",
        "Retriever will embed input question (as well as my previously generated alternatives) with the same `llm_emb` model as was using for vectorstore and will provide `top_k` documents similar to the question by `search_type`.\n",
        "\n",
        "Values of `top_k` and `search_type` were provided in calling `vectorstore.as_retriever()` above."
      ],
      "metadata": {
        "id": "vFmecMDYk1MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieval Chain for multiple alternatives to the question formulation\n",
        "retrieval_chain = (\n",
        "    # We can attach previous chains as input to the next chain:\n",
        "    invoke_generate_queries_chain\n",
        "    # Next step is retriever. Here we need to split str with alternative multiple queries into list to\n",
        "    # allow retriever to deal with them separatedlly and calling .map() function.\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        "    | retriever.map()\n",
        "    | invoke_unique_docs_union_from_retriever\n",
        ")\n",
        "# to check list of retrieved documents\n",
        "result = retrieval_chain.invoke({\"question\": Config.MYQ, \"question_numbers\": 2})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "uaB5s6Gnk1Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt for generation answer with retriever and generation prompt"
      ],
      "metadata": {
        "id": "H5EaPovVlqtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Prompt for generation answer with retriever and generation prompt\n",
        "prompt_template_question_context = \"\"\"\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "Please follow the following rules:\n",
        "1. If you don't know the answer, don't try to make up an answer.\n",
        "Just say \"I can't find the final answer but you may want to check the following links\".\n",
        "2. If you find the answer, write the answer in a concise way with five sentences maximum.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "*** Helpful Answer***:\n",
        "\"\"\"\n",
        "\n",
        "prompt_generation = PromptTemplate(\n",
        "    template=prompt_template_question_context,\n",
        "    input_variables=[\"question\", \"context\"],\n",
        ")"
      ],
      "metadata": {
        "id": "8I6Q4pVGlq2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Chain\n",
        "-  Generator (could be another model as for retriever) takes list of retrieved (relevant) documents and generate answer for the qustion according to them."
      ],
      "metadata": {
        "id": "vcI0DIKIlu9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG Chain\n",
        "# Generator (could be another model as for retriever) takes list of retrieved (relevant) documents and generate\n",
        "# answer for the qustion according to them.\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": retrieval_chain,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    # Here again: since prompt_generation takes as input 2 variables with names: context and question,\n",
        "    # we assign these name to the variables as dict keys.\n",
        "    # \"context\" will take value from the output of retrieval_chain\n",
        "    # \"question\" will take value from calling this.invoke() with provided \"question\" key\n",
        "    | prompt_generation\n",
        "    | llm_gen\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke({\"question\": Config.MYQ, \"question_numbers\": 2})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "UnbXEwyIlx8S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}